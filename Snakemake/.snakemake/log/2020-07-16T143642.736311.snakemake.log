Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	get_features
	1	run_parSMURF
	3

[Thu Jul 16 14:36:42 2020]
rule get_features:
    input: Mendelian.txt
    output: features/.txt, folds/.txt, labels/.txt
    jobid: 2

[Thu Jul 16 14:36:42 2020]
Finished job 2.
1 of 3 steps (33%) done

[Thu Jul 16 14:36:42 2020]
rule run_parSMURF:
    input: features/.txt, folds/.txt, labels/.txt
    output: temp.txt
    jobid: 1

[Thu Jul 16 14:36:44 2020]
Finished job 1.
2 of 3 steps (67%) done

[Thu Jul 16 14:36:44 2020]
localrule all:
    input: temp.txt
    jobid: 0

[Thu Jul 16 14:36:44 2020]
Finished job 0.
3 of 3 steps (100%) done
Complete log: /Users/Lusine/Documents/Masterarbeit/Code/MA/Snakemake/.snakemake/log/2020-07-16T143642.736311.snakemake.log
