Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	get_features
	1	run_parSMURF
	3

[Thu Jul 16 14:20:37 2020]
rule get_features:
    input: Mendelian.txt
    output: features.txt, folds.txt, labels.txt
    jobid: 2

[Thu Jul 16 14:20:37 2020]
Error in rule get_features:
    jobid: 2
    output: features.txt, folds.txt, labels.txt
    shell:
        
		cut -d$'	' -f 1-26 < Mendelian.txt > features.txt folds.txt labels.txt;
		cut -f27 < Mendelian.txt > features.txt folds.txt labels.txt;
		cut -f28 < Mendelian.txt > features.txt folds.txt labels.txt

		
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job get_features since they might be corrupted:
features.txt
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /Users/Lusine/Documents/Masterarbeit/Code/MA/Snakemake/.snakemake/log/2020-07-16T142037.163328.snakemake.log
